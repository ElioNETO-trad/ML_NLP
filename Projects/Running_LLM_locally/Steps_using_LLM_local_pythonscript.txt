1 - Have Ollama with a LLM model of your choice installed and certify that is running.

2 - Check if the model runs and receives an API response (curl and receives)

MODEL_NAME = Name of the model you want to use
API_URL = localhost:[Port Number]/api/generate

curl -X POST [API_URL] -H "Content-Type: application/json" -d "{\"model\": \"[MODEL_NAME]\", \"prompt\": \"Hello\", \"stream\": false}"

3 - Use the following script.

4 - Import the specified module to another script and check if it's working.


